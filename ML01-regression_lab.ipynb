{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习练习 - 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码汇总于github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单变量线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/regress_data1.csv'\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # 查看数据前几行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail() # 查看数据后几行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns # 查看数据列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape # 查看数据形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum() # 查看数据重复值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all') # 查看数据描述性统计，包含所有列类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看下数据长什么样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制散点图\n",
    "# 确保'人口'和'收益'列为float类型，避免dtype错误\n",
    "data['人口'] = data['人口'].astype(float, errors='ignore')\n",
    "data['收益'] = data['收益'].astype(float, errors='ignore')\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(data['人口'], data['收益'])\n",
    "# 设置x轴标签\n",
    "plt.xlabel('人口', fontsize=18)\n",
    "# 设置y轴标签，并将标签旋转为水平方向\n",
    "plt.ylabel('收益', rotation=0, fontsize=18)\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们使用梯度下降来实现线性回归，以最小化代价函数。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们将创建一个以参数$w$为特征函数的代价函数\n",
    "$$J\\left( w  \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {{h}}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}} \\right)}^{2}}}$$\n",
    "其中：$${{h}}\\left( x \\right)={{w}^{T}}X={{w }_{0}}{{x}_{0}}+{{w }_{1}}{{x}_{1}}+{{w }_{2}}{{x}_{2}}+...+{{w }_{n}}{{x}_{n}}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, w):\n",
    "    \"\"\"\n",
    "    计算线性回归模型的代价函数。    \n",
    "    参数：\n",
    "    X -- 特征矩阵，形状为 (n_samples, n_features)\n",
    "    y -- 标签向量，形状为 (n_samples,1)\n",
    "    w -- 权重向量，形状为 (n_features,1)\n",
    "    \n",
    "    返回值：\n",
    "    代价函数的值\n",
    "    \"\"\"\n",
    "    # TODO: 计算 (X @ w - y) 的平方并求和，再除以 2m 得到代价\n",
    "    # 提示：m = len(X)；可用 np.power 或 **2，矩阵乘法用 @\n",
    "    inner = ___\n",
    "    return ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们在训练集中添加一列，以便我们可以使用向量化的解决方案来计算代价和梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将一列名为'Ones'的值全为1的列插入到data的第一列位置。\n",
    "data.insert(0, 'Ones', 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来做一些变量初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.shape[1]  # 获取data的列数\n",
    "X = data.iloc[:,:cols-1]  # 获取除最后一列外的所有列作为特征矩阵X\n",
    "y = data.iloc[:,cols-1:]  # 获取最后一列作为目标变量y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察下 X (训练集) and y (目标变量)是否正确."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head()#head()是观察前5行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代价函数是应该是numpy矩阵，所以我们需要转换X和Y，然后才能使用它们。 我们还需要初始化w。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "# TODO: 初始化参数向量 w 为全零，形状为 (n_features, 1)\n",
    "# 提示：n_features 可用 X.shape[1]\n",
    "w = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w 现在是一个形状为 (2, 1) 的二维数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看下维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, w.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算代价函数 (theta初始值为0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeCost(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Decent（批量梯度下降）\n",
    "\n",
    "$${{w }_{j}}:={{w }_{j}}- \\alpha \\frac{1}{m}\\sum\\limits_{i=1}^m \\frac{\\partial }{\\partial {{w}_{j}}}J\\left( w \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradientDescent(X, y, w, alpha, count):\n",
    "    \"\"\"\n",
    "    批量梯度下降算法实现。\n",
    "    \n",
    "    参数：\n",
    "    X -- 特征矩阵，形状为 (n_samples, n_features)\n",
    "    y -- 标签向量，形状为 (n_samples,1)\n",
    "    w -- 权重向量，形状为 (n_features,1)\n",
    "    alpha -- 学习率\n",
    "    count -- 迭代次数\n",
    "    \n",
    "    返回值：\n",
    "    w -- 更新后的权重向量\n",
    "    costs -- 每次迭代的代价函数值列表\n",
    "    \"\"\"\n",
    "    # 初始化代价函数值列表\n",
    "    costs = []\n",
    "\n",
    "    # 对每个样本进行迭代\n",
    "    for i in range(count):\n",
    "        # TODO: 使用向量化的批量梯度下降更新 w\n",
    "        # 提示：梯度为 X.T @ (X @ w - y)，别忘了除以 m=len(X)\n",
    "        w = ___\n",
    "\n",
    "        # 计算当前代价函数值并添加到列表中\n",
    "        cost = computeCost(X, y, w)\n",
    "        costs.append(cost)\n",
    "\n",
    "        # 每隔100次迭代输出一次当前代价函数值\n",
    "        if i % 100 == 0:\n",
    "            print(\"在第{}次迭代中，cost的值是：{}。\".format(i, cost))\n",
    "\n",
    "    # 返回最终的权重向量和代价函数值列表\n",
    "    return w, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化一些附加变量 - 学习速率α和要执行的迭代次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们运行梯度下降算法来将我们的参数θ适合于训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, cost = batch_gradientDescent(X, y, w, alpha, iters)#返回更新后的参数向量g和损失值数组cost。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以使用我们拟合的参数计算训练模型的代价函数（误差）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeCost(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来绘制线性模型以及数据，直观地看出它的拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成预测值\n",
    "x = np.linspace(data['人口'].min(), data['人口'].max(), 100)\n",
    "# 根据参数估计值生成预测值\n",
    "f = w[0, 0] + (w[1, 0] * x)\n",
    "# 创建图形和轴对象\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# 绘制预测值曲线\n",
    "ax.plot(x, f, 'r', label='预测值')\n",
    "# 绘制训练数据散点图\n",
    "ax.scatter(data['人口'], data['收益'], label='训练数据')\n",
    "# 添加图例\n",
    "ax.legend(loc=2)\n",
    "# 设置x轴和y轴标签\n",
    "ax.set_xlabel('人口', fontsize=18)\n",
    "ax.set_ylabel('收益', rotation=0, fontsize=18)\n",
    "# 设置图标题\n",
    "ax.set_title('预测收益和人口规模', fontsize=18)\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于梯度方程式函数也在每个训练迭代中输出一个代价的向量，所以我们也可以绘制。 请注意，代价总是降低 - 这是凸优化问题的一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建图形和轴对象\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# 绘制代价随迭代次数的变化曲线\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "# 设置x轴和y轴标签\n",
    "ax.set_xlabel('迭代次数', fontsize=18)\n",
    "ax.set_ylabel('代价', rotation=0, fontsize=18)\n",
    "# 设置图标题\n",
    "ax.set_title('误差和训练Epoch数', fontsize=18)\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多变量线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习还包括一个房屋价格数据集，其中有2个变量（房子的大小，卧室的数量）和目标（房子的价格）。 我们使用我们已经应用的技术来分析数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/regress_data2.csv'\n",
    "data2 = pd.read_csv(path)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于此任务，我们添加了另一个预处理步骤 - 特征归一化。 这个对于pandas来说很简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 对 data2 做特征标准化：(x - 均值) / 标准差\n",
    "# 提示：可直接用 DataFrame 的 mean() 和 std()\n",
    "data2 = ___\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们重复第1部分的预处理步骤，并对新数据集运行线性回归程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加一列全为1的列\n",
    "data2.insert(0, 'Ones', 1)\n",
    "# 设置X（训练数据）和y（目标变量）\n",
    "cols = data2.shape[1]\n",
    "X2 = data2.iloc[:, 0:cols - 1]\n",
    "y2 = data2.iloc[:, cols - 1:cols]\n",
    "# 转换为数组并初始化theta\n",
    "X2 = X2.values\n",
    "y2 = y2.values\n",
    "# TODO: 初始化 w2 为合适形状的全零向量\n",
    "# 提示：根据特征数设置形状，可用 X2.shape[1]\n",
    "w2 = ___\n",
    "# 在数据集上进行线性回归\n",
    "w2, cost2 = batch_gradientDescent(X2, y2, w2, alpha, iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取模型的代价（误差）\n",
    "computeCost(X2, y2, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以快速查看这一个的训练进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 创建一个大小为 (12, 8) 的画布和坐标轴对象\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# 在坐标轴上绘制代价函数 cost2 随迭代次数 iters 的变化曲线，使用红色线条\n",
    "ax.plot(np.arange(iters), cost2, 'r')\n",
    "# 设置 x 轴标签为 \"迭代次数\",字体大小为 18\n",
    "ax.set_xlabel('迭代次数', fontsize=18)\n",
    "# 设置 y 轴标签为 \"代价\",旋转角度为 0,字体大小为 18\n",
    "ax.set_ylabel('代价', rotation=0, fontsize=18)\n",
    "# 设置图表标题为 \"误差和训练Epoch数\",字体大小为 18\n",
    "ax.set_title('误差和训练Epoch数', fontsize=18)\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以使用scikit-learn的线性回归函数，而不是从头开始实现这些算法。 我们将scikit-learn的线性回归算法应用于第1部分的数据，并看看它的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入线性回归模型\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 创建线性回归模型对象\n",
    "model = LinearRegression()\n",
    "\n",
    "# 使用训练数据 X 和标签 y 对模型进行拟合\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn model的预测表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 取特征矩阵 X 的第二列作为自变量 x\n",
    "x = X[:, 1]\n",
    "# 对模型进行预测，并将预测结果展平为一维数组 f\n",
    "f = model.predict(X).flatten()\n",
    "# 创建画布和坐标轴对象\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# 在坐标轴上绘制预测值 f 随人口规模 x 的变化曲线，使用红色线条，并添加标签\n",
    "ax.plot(x, f, 'r', label='预测值')\n",
    "# 在坐标轴上绘制训练数据点的散点图，并添加标签\n",
    "ax.scatter(data['人口'], data['收益'], label='训练数据')\n",
    "# 显示图例，并设置位置和字体大小\n",
    "ax.legend(loc=2, fontsize=18)\n",
    "# 设置 x 轴标签和字体大小\n",
    "ax.set_xlabel('人口', fontsize=18)\n",
    "# 设置 y 轴标签、旋转角度和字体大小\n",
    "ax.set_ylabel('收益', rotation=0, fontsize=18)\n",
    "# 设置图表标题和字体大小\n",
    "ax.set_title('预测收益和人口规模', fontsize=18)\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L_2$正则化\n",
    "$J (  { w } ) = \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { m } ( h _ { w} ( x ^ { ( i ) } ) - y ^ { ( i ) } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { n } w_ { j } ^ { 2 }$，此时称作`Ridge Regression`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge# 导入岭回归模型\n",
    "# TODO: 为 Ridge 指定正则化强度 alpha（如 1.0）\n",
    "model = Ridge(alpha=___)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x2 = X[:, 1]\n",
    "f2 = model.predict(X).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(x2, f2, 'r', label='预测值Ridge')\n",
    "ax.scatter(data['人口'], data['收益'], label='训练数据')\n",
    "ax.legend(loc=2, fontsize=18)\n",
    "ax.set_xlabel('人口', fontsize=18)\n",
    "ax.set_ylabel('收益', rotation=0, fontsize=18)\n",
    "ax.set_title('预测收益和人口规模', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L_1$正则化：\n",
    "$J (  {w } ) = \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { m } ( h _ { w} ( x ^ { ( i ) } ) - y ^ { ( i ) } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { n } | w _ { j } |$，此时称作`Lasso Regression` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso# 导入 Lasso 回归模型\n",
    "# TODO: 为 Lasso 指定正则化强度 alpha（如 0.1）\n",
    "model = Lasso(alpha=___)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3= X[:, 1]\n",
    "f3 = model.predict(X).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(x3, f3, 'r', label='预测值Lasso')\n",
    "ax.scatter(data['人口'], data['收益'], label='训练数据')\n",
    "ax.legend(loc=2, fontsize=18)\n",
    "ax.set_xlabel('人口', fontsize=18)\n",
    "ax.set_ylabel('收益', rotation=0, fontsize=18)\n",
    "ax.set_title('预测收益和人口规模', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库和模块\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 定义一组不同的alpha值\n",
    "alphas = np.logspace(-3, 2, 50)\n",
    "# 初始化一个空列表，用于存储测试分数\n",
    "test_scores = []\n",
    "# 遍历每个alpha值\n",
    "for alpha in alphas:\n",
    "    # 创建一个Ridge回归模型，设置alpha值\n",
    "    clf = Ridge(alpha)    \n",
    "    # 使用交叉验证计算负均方误差的平方根，并取平均值\n",
    "    test_score = np.sqrt(-cross_val_score(clf, X, y, cv=5, scoring='neg_mean_squared_error'))    \n",
    "    # 将平均测试分数添加到列表中\n",
    "    test_scores.append(np.mean(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, test_scores)\n",
    "plt.title(\"Alpha vs CV Error\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小二乘法(LSM)：\n",
    "\n",
    "最小二乘法的需要求解最优参数$w^{*}$：\n",
    "\n",
    "已知：目标函数\n",
    "\n",
    "$J\\left( w  \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {h}\\left( {x^{(i)}} \\right)-{y^{(i)}} \\right)}^{2}}}$\n",
    "\n",
    "其中：${h}\\left( x \\right)={w^{T}}X={w_{0}}{x_{0}}+{w_{1}}{x_{1}}+{w_{2}}{x_{2}}+...+{w_{n}}{x_{n}}$\n",
    "\n",
    "将向量表达形式转为矩阵表达形式，则有$J(w )=\\frac{1}{2}{{\\left( Xw -y\\right)}^{2}}$ ，其中$X$为$m$行$n+1$列的矩阵（$m$为样本个数，$n$为特征个数），$w$为$n+1$行1列的矩阵(包含了$w_0$)，$y$为$m$行1列的矩阵，则可以求得最优参数$w^{*} ={{\\left( {X^{T}}X \\right)}^{-1}}{X^{T}}y$ \n",
    "\n",
    "梯度下降与最小二乘法的比较：\n",
    "\n",
    "梯度下降：需要选择学习率$\\alpha$，需要多次迭代，当特征数量$n$大时也能较好适用，适用于各种类型的模型\t\n",
    "\n",
    "最小二乘法：不需要选择学习率$\\alpha$，一次计算得出，需要计算${{\\left( {{X}^{T}}X \\right)}^{-1}}$，如果特征数量$n$较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当$n$小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSM(X, y):\n",
    "    # 计算矩阵X的转置与X相乘，再求逆矩阵，最后将结果与X的转置相乘，再与向量y相乘，得到权重w\n",
    "    # TODO: 使用正规方程计算 w = (X^T X)^{-1} X^T y\n",
    "    # 提示：矩阵乘法用 @，求逆用 np.linalg.inv\n",
    "    w = ___\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_w2=LSM(X, y)#感觉和批量梯度下降的theta的值有点差距\n",
    "final_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到梯度下降得到的结果是：\n",
    "\n",
    "array([[-3.78806857],\n",
    "  \n",
    "[ 1.18221277]])\n",
    "       \n",
    "       与最小二乘法的结果非常接近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 参考\n",
    "[1] Andrew Ng. Machine Learning[EB/OL]. StanfordUniversity,2014.https://www.coursera.org/course/ml\n",
    "\n",
    "[2] 李航. 统计学习方法[M]. 北京: 清华大学出版社,2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
